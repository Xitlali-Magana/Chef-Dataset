{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "575c85eb",
   "metadata": {},
   "source": [
    "<hr style=\"height:.6px;color:#333;\" />\n",
    "<h1><b>Classification Model Development</b></h1>\n",
    "<h2>Xitlali Magana</h2>\n",
    "<br>\n",
    "<hr style=\"height:.3px;color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc9d60f",
   "metadata": {},
   "source": [
    "This code imports various Python libraries for data analysis and machine learning, including NumPy, Pandas, Matplotlib, Seaborn, Scikit-learn (sklearn), and Statsmodels. It then loads a dataset named 'Cross_Sell_Success_Dataset_2023.xlsx' into a Pandas DataFrame named 'chef'. The code also sets some Pandas print options for displaying the data and shows the first five rows of the dataset using the 'head()' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f008e212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CROSS_SELL_SUCCESS</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>REVENUE</th>\n",
       "      <th>TOTAL_MEALS_ORDERED</th>\n",
       "      <th>UNIQUE_MEALS_PURCH</th>\n",
       "      <th>CONTACTS_W_CUSTOMER_SERVICE</th>\n",
       "      <th>PRODUCT_CATEGORIES_VIEWED</th>\n",
       "      <th>AVG_TIME_PER_SITE_VISIT</th>\n",
       "      <th>CANCELLATIONS_AFTER_NOON</th>\n",
       "      <th>PC_LOGINS</th>\n",
       "      <th>MOBILE_LOGINS</th>\n",
       "      <th>WEEKLY_PLAN</th>\n",
       "      <th>LATE_DELIVERIES</th>\n",
       "      <th>AVG_PREP_VID_TIME</th>\n",
       "      <th>LARGEST_ORDER_SIZE</th>\n",
       "      <th>AVG_MEAN_RATING</th>\n",
       "      <th>TOTAL_PHOTOS_VIEWED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>steffon.baratheon@yahoo.com</td>\n",
       "      <td>4920.0</td>\n",
       "      <td>493</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>265.6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>137.41</td>\n",
       "      <td>6</td>\n",
       "      <td>2.894737</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>harlon.greyjoy@visa.com</td>\n",
       "      <td>6150.0</td>\n",
       "      <td>361</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>247.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120.20</td>\n",
       "      <td>5</td>\n",
       "      <td>2.631579</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>monster@protonmail.com</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>278</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>164.4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>127.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>damon.lannister.(lord)@yahoo.com</td>\n",
       "      <td>3330.0</td>\n",
       "      <td>269</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>176.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>129.78</td>\n",
       "      <td>6</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>raynald.westerling@jnj.com</td>\n",
       "      <td>3427.5</td>\n",
       "      <td>276</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>164.6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>34.42</td>\n",
       "      <td>3</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CROSS_SELL_SUCCESS                             EMAIL  REVENUE  TOTAL_MEALS_ORDERED  UNIQUE_MEALS_PURCH  CONTACTS_W_CUSTOMER_SERVICE  PRODUCT_CATEGORIES_VIEWED  AVG_TIME_PER_SITE_VISIT  CANCELLATIONS_AFTER_NOON  PC_LOGINS  MOBILE_LOGINS  WEEKLY_PLAN  LATE_DELIVERIES   AVG_PREP_VID_TIME  LARGEST_ORDER_SIZE  AVG_MEAN_RATING  TOTAL_PHOTOS_VIEWED\n",
       "0                   1       steffon.baratheon@yahoo.com   4920.0                  493                   9                            1                         10                    265.6                         5          5              2            0                 0             137.41                   6         2.894737                  456\n",
       "1                   0           harlon.greyjoy@visa.com   6150.0                  361                   9                            1                          6                    247.0                         2          5              1            0                 0             120.20                   5         2.631579                  680\n",
       "2                   0            monster@protonmail.com   3435.0                  278                   6                            1                          4                    164.4                         0          6              1            5                 0             127.00                   3         3.684211                  145\n",
       "3                   1  damon.lannister.(lord)@yahoo.com   3330.0                  269                   8                            1                          2                    176.0                         5          5              2            0                 0             129.78                   6         3.157895                  418\n",
       "4                   1        raynald.westerling@jnj.com   3427.5                  276                   7                            1                         10                    164.6                         0          6              1           14                 0              34.42                   3         3.157895                  174"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns    \n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression    \n",
    "from sklearn.metrics import confusion_matrix         \n",
    "from sklearn.metrics import roc_auc_score          \n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.neighbors import KNeighborsRegressor   \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# loading data\n",
    "chef = pd.read_excel(io = 'Cross_Sell_Success_Dataset_2023.xlsx')\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "# displaying the head of the dataset\n",
    "chef.head(n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a578b8",
   "metadata": {},
   "source": [
    "This code is performing data cleaning and preparation for analysis on the chef dataset. It begins by checking for missing values in each feature, and then renaming the \"LATE_DELIVERIES \" column to \"LATE_DELIVERIES\". The next section creates a histogram for each column in the dataset to visualize the distribution of the data. The code then performs log transformations on a list of columns that appear to be skewed. Finally, useless features are dropped from the dataset, including duplicates, and the results are checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9219b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### data cleaning ############################\n",
    "# checking each feature for missing values\n",
    "chef.isnull().sum().round(decimals=2)\n",
    "# checking column names\n",
    "chef.columns\n",
    "# renaming the late deliveries column to remove the space\n",
    "chef = chef.rename(columns={\"LATE_DELIVERIES \":\"LATE_DELIVERIES\"})\n",
    "\n",
    "####################### HISTPLOTS ############################\n",
    "# Iterating over the columns in the chef DataFrame\n",
    "\"\"\" (commenting out becuase only needed for visualization)\n",
    "for col in chef.columns:\n",
    "    # creating a histplot for every column\n",
    "    sns.histplot(data=chef, \n",
    "                 x=col, \n",
    "                 kde=False)\n",
    "    # adding a title to each histogram\n",
    "    plt.title(f\"Histogram of {col}\")\n",
    "    # displaying the histogram\n",
    "    # plt.show() \n",
    "\"\"\"\n",
    "    \n",
    "####################### LOG TRANSFORMATIONS ############################\n",
    "# list of columns that appear to be skewed\n",
    "skewed_columns = ['REVENUE', 'TOTAL_MEALS_ORDERED', 'PRODUCT_CATEGORIES_VIEWED',\n",
    "                  'CANCELLATIONS_AFTER_NOON', 'WEEKLY_PLAN', 'LATE_DELIVERIES',\n",
    "                 'TOTAL_PHOTOS_VIEWED']\n",
    "# for loop to create a new column for each skewed column with a log transformation\n",
    "for col in skewed_columns:\n",
    "    if col in chef.columns:\n",
    "        chef['log_' + col] = np.log(chef[col] + 0.001)\n",
    "        \n",
    "########################### DATA DROP ############################## \n",
    "# dropping useless features, and duplicates\n",
    "chef = chef.drop(['EMAIL', 'REVENUE', 'TOTAL_MEALS_ORDERED', 'PRODUCT_CATEGORIES_VIEWED',\n",
    "                'CANCELLATIONS_AFTER_NOON', 'WEEKLY_PLAN', 'LATE_DELIVERIES', \n",
    "                'TOTAL_PHOTOS_VIEWED'],\n",
    "               axis = 1)\n",
    "# checking results\n",
    "# chef.columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a1c82",
   "metadata": {},
   "source": [
    "First, the correlation between the customer features and the target variable \"CROSS_SELL_SUCCESS\" is computed using the Pearson correlation method, with the result rounded to 2 decimal places. The correlation coefficients are then sorted in descending order based on their absolute values, in order to identify which features have the strongest positive and negative correlation with \"CROSS_SELL_SUCCESS\".\n",
    "\n",
    "Next, a list of explanatory variables (\"x_var\") is created, which includes the features that will be used as predictors in the logistic regression model. The explanatory variables and target variable are then separated into their own dataframes (\"x_data\" and \"y_var\", respectively).\n",
    "\n",
    "The data is then split into training and testing sets using the train_test_split function, with a test size of 10%, a random state of 219 (for reproducibility), and stratification to preserve the balance of the target variable in both the training and testing sets.\n",
    "\n",
    "The training data is then merged into a single dataframe (\"chef_train\") for use in the logistic regression model.\n",
    "\n",
    "Finally, a logistic regression model is instantiated using statsmodels, with the explanatory variables and target variable specified using a formula. The model is then fit using the training data, and the results summary is printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1d19bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.613127\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>Logit</td>       <td>Pseudo R-squared:</td>    <td>0.023</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td> <td>CROSS_SELL_SUCCESS</td>       <td>AIC:</td>         <td>2179.1693</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>         <td>2023-03-14 13:25</td>        <td>BIC:</td>         <td>2266.6564</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>1751</td>         <td>Log-Likelihood:</td>    <td>-1073.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>15</td>             <td>LL-Null:</td>        <td>-1098.9</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>1735</td>          <td>LLR p-value:</td>    <td>9.4146e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>            <td>1.0000</td>            <td>Scale:</td>         <td>1.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>         <td>5.0000</td>               <td></td>               <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                <td></td>                 <th>Coef.</th>  <th>Std.Err.</th>    <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                     <td>2.1596</td>   <td>1.4821</td>  <td>1.4571</td>  <td>0.1451</td> <td>-0.7453</td> <td>5.0645</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UNIQUE_MEALS_PURCH</th>            <td>0.0572</td>   <td>0.0252</td>  <td>2.2683</td>  <td>0.0233</td> <td>0.0078</td>  <td>0.1067</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CONTACTS_W_CUSTOMER_SERVICE</th>   <td>0.0283</td>   <td>0.0223</td>  <td>1.2677</td>  <td>0.2049</td> <td>-0.0154</td> <td>0.0720</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_TIME_PER_SITE_VISIT</th>       <td>0.0043</td>   <td>0.0023</td>  <td>1.8226</td>  <td>0.0684</td> <td>-0.0003</td> <td>0.0088</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PC_LOGINS</th>                     <td>0.1624</td>   <td>0.0894</td>  <td>1.8176</td>  <td>0.0691</td> <td>-0.0127</td> <td>0.3376</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MOBILE_LOGINS</th>                 <td>0.2633</td>   <td>0.1004</td>  <td>2.6218</td>  <td>0.0087</td> <td>0.0665</td>  <td>0.4602</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_PREP_VID_TIME</th>             <td>0.0004</td>   <td>0.0009</td>  <td>0.4583</td>  <td>0.6467</td> <td>-0.0014</td> <td>0.0022</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LARGEST_ORDER_SIZE</th>            <td>-0.0598</td>  <td>0.0595</td>  <td>-1.0055</td> <td>0.3147</td> <td>-0.1764</td> <td>0.0568</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_MEAN_RATING</th>               <td>-0.1243</td>  <td>0.1075</td>  <td>-1.1563</td> <td>0.2475</td> <td>-0.3350</td> <td>0.0864</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_REVENUE</th>                   <td>-0.5586</td>  <td>0.1976</td>  <td>-2.8271</td> <td>0.0047</td> <td>-0.9458</td> <td>-0.1713</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_TOTAL_MEALS_ORDERED</th>       <td>0.2808</td>   <td>0.1131</td>  <td>2.4834</td>  <td>0.0130</td> <td>0.0592</td>  <td>0.5024</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_PRODUCT_CATEGORIES_VIEWED</th> <td>0.0320</td>   <td>0.0736</td>  <td>0.4353</td>  <td>0.6633</td> <td>-0.1121</td> <td>0.1762</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_CANCELLATIONS_AFTER_NOON</th>  <td>0.0681</td>   <td>0.0146</td>  <td>4.6534</td>  <td>0.0000</td> <td>0.0394</td>  <td>0.0968</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_WEEKLY_PLAN</th>               <td>-0.0087</td>  <td>0.0130</td>  <td>-0.6722</td> <td>0.5015</td> <td>-0.0342</td> <td>0.0168</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_LATE_DELIVERIES</th>           <td>0.0075</td>   <td>0.0135</td>  <td>0.5567</td>  <td>0.5777</td> <td>-0.0189</td> <td>0.0340</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_TOTAL_PHOTOS_VIEWED</th>       <td>-0.0060</td>  <td>0.0366</td>  <td>-0.1635</td> <td>0.8701</td> <td>-0.0777</td> <td>0.0657</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                               Results: Logit\n",
       "=============================================================================\n",
       "Model:                  Logit                 Pseudo R-squared:    0.023     \n",
       "Dependent Variable:     CROSS_SELL_SUCCESS    AIC:                 2179.1693 \n",
       "Date:                   2023-03-14 13:25      BIC:                 2266.6564 \n",
       "No. Observations:       1751                  Log-Likelihood:      -1073.6   \n",
       "Df Model:               15                    LL-Null:             -1098.9   \n",
       "Df Residuals:           1735                  LLR p-value:         9.4146e-06\n",
       "Converged:              1.0000                Scale:               1.0000    \n",
       "No. Iterations:         5.0000                                               \n",
       "-----------------------------------------------------------------------------\n",
       "                               Coef.  Std.Err.    z    P>|z|   [0.025  0.975]\n",
       "-----------------------------------------------------------------------------\n",
       "Intercept                      2.1596   1.4821  1.4571 0.1451 -0.7453  5.0645\n",
       "UNIQUE_MEALS_PURCH             0.0572   0.0252  2.2683 0.0233  0.0078  0.1067\n",
       "CONTACTS_W_CUSTOMER_SERVICE    0.0283   0.0223  1.2677 0.2049 -0.0154  0.0720\n",
       "AVG_TIME_PER_SITE_VISIT        0.0043   0.0023  1.8226 0.0684 -0.0003  0.0088\n",
       "PC_LOGINS                      0.1624   0.0894  1.8176 0.0691 -0.0127  0.3376\n",
       "MOBILE_LOGINS                  0.2633   0.1004  2.6218 0.0087  0.0665  0.4602\n",
       "AVG_PREP_VID_TIME              0.0004   0.0009  0.4583 0.6467 -0.0014  0.0022\n",
       "LARGEST_ORDER_SIZE            -0.0598   0.0595 -1.0055 0.3147 -0.1764  0.0568\n",
       "AVG_MEAN_RATING               -0.1243   0.1075 -1.1563 0.2475 -0.3350  0.0864\n",
       "log_REVENUE                   -0.5586   0.1976 -2.8271 0.0047 -0.9458 -0.1713\n",
       "log_TOTAL_MEALS_ORDERED        0.2808   0.1131  2.4834 0.0130  0.0592  0.5024\n",
       "log_PRODUCT_CATEGORIES_VIEWED  0.0320   0.0736  0.4353 0.6633 -0.1121  0.1762\n",
       "log_CANCELLATIONS_AFTER_NOON   0.0681   0.0146  4.6534 0.0000  0.0394  0.0968\n",
       "log_WEEKLY_PLAN               -0.0087   0.0130 -0.6722 0.5015 -0.0342  0.0168\n",
       "log_LATE_DELIVERIES            0.0075   0.0135  0.5567 0.5777 -0.0189  0.0340\n",
       "log_TOTAL_PHOTOS_VIEWED       -0.0060   0.0366 -0.1635 0.8701 -0.0777  0.0657\n",
       "=============================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################### CORRELATION ############################## \n",
    "# correlation of the features\n",
    "chef_corr = chef.corr(method=\"pearson\").round(decimals =2)\n",
    "chef_corr['CROSS_SELL_SUCCESS'].sort_values(ascending = False)\n",
    "\n",
    "########################### PREPARING DATA ############################## \n",
    "# list of x variables\n",
    "x_var = ['UNIQUE_MEALS_PURCH', 'CONTACTS_W_CUSTOMER_SERVICE', 'AVG_TIME_PER_SITE_VISIT', \n",
    "          'PC_LOGINS', 'MOBILE_LOGINS', 'AVG_PREP_VID_TIME', 'LARGEST_ORDER_SIZE', \n",
    "          'AVG_MEAN_RATING', 'log_REVENUE', 'log_TOTAL_MEALS_ORDERED', \n",
    "          'log_PRODUCT_CATEGORIES_VIEWED', 'log_CANCELLATIONS_AFTER_NOON', \n",
    "          'log_WEEKLY_PLAN', 'log_LATE_DELIVERIES', 'log_TOTAL_PHOTOS_VIEWED']\n",
    "# declare explanatory variables (x)\n",
    "x_data = chef[x_var]\n",
    "# declare response variable (y)\n",
    "y_var = chef.loc[:, 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "########################### TEST/TRAIN SPLIT ############################## \n",
    "# train-test split including stratification\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            x_data,\n",
    "            y_var,\n",
    "            test_size = 0.10,\n",
    "            random_state = 219,\n",
    "            stratify = y_var) # preserving balance\n",
    "# merge training data for statsmodels\n",
    "chef_train = pd.concat([x_train, y_train], axis = 1)\n",
    "\n",
    "########################### BASE MODEL ############################## \n",
    "# instantiating a logistic regression model object\n",
    "logistic = smf.logit(formula = \"\"\" CROSS_SELL_SUCCESS ~ UNIQUE_MEALS_PURCH +\n",
    "                                                        CONTACTS_W_CUSTOMER_SERVICE +\n",
    "                                                        AVG_TIME_PER_SITE_VISIT +\n",
    "                                                        PC_LOGINS +\n",
    "                                                        MOBILE_LOGINS +\n",
    "                                                        AVG_PREP_VID_TIME +\n",
    "                                                        LARGEST_ORDER_SIZE +\n",
    "                                                        AVG_MEAN_RATING +\n",
    "                                                        log_REVENUE +\n",
    "                                                        log_TOTAL_MEALS_ORDERED +\n",
    "                                                        log_PRODUCT_CATEGORIES_VIEWED +\n",
    "                                                        log_CANCELLATIONS_AFTER_NOON +\n",
    "                                                        log_WEEKLY_PLAN +\n",
    "                                                        log_LATE_DELIVERIES +\n",
    "                                                        log_TOTAL_PHOTOS_VIEWED\n",
    "                                                     \"\"\",\n",
    "                                        data = chef_train)\n",
    "# fitting the model object\n",
    "results = logistic.fit()\n",
    "# checking the results SUMMARY\n",
    "results.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3465b78",
   "metadata": {},
   "source": [
    "These are a series of feature engineering steps applied to the 'chef' dataset. Feature engineering is the process of transforming raw data into useful features that can be used to improve the performance of machine learning algorithms. Here is a brief explanation of each feature that has been created:\n",
    "\n",
    "unique_per_mobile: calculates the number of unique meal purchases per mobile login. This could be an interesting metric to understand if customers who use mobile tend to be more adventurous in their meal choices compared to those who use a desktop. It could also be an indicator of convenience or ease of ordering via mobile.\n",
    "\n",
    "cancel_per_unique: calculates the number of unique meals purchased to cancellations after noon. This could help identify if customers who cancel after the cutoff time tend to order more unique or complex meals. It could also help identify if there is a correlation between cancellations and dissatisfaction with menu options.\n",
    "\n",
    "mobile_per_total: calculates the number of mobile logins per total meals ordered. This metric could indicate if mobile users tend to order more or less frequently than desktop users. It could also help identify if there is a correlation between convenience and order frequency.\n",
    "\n",
    "total_to_revenue: calculates how much revenue per meal ordered. This could provide insights into the average amount of revenue generated per order. It could also be an indicator of pricing strategy effectiveness.\n",
    "\n",
    "total_to_cancel: calculates the number of meals purchased to cancellations after noon. This could help identify if there is a correlation between meal complexity and cancellations after the cutoff time. It could also help identify if there is a correlation between dissatisfaction with menu options and cancellations.\n",
    "\n",
    "mobile_pc: calculates the number of mobile logins to pc logins. This could be an interesting metric to understand if customers tend to use one platform over the other. It could also help identify if there is a correlation between platform usage and order frequency.\n",
    "\n",
    "total_to_late: calculates the number of meals ordered per late delivery. This could help identify if customers who experience late deliveries tend to order more frequently or if there is a correlation between dissatisfaction with delivery times and order frequency.\n",
    "\n",
    "time_vs_average: This ratio measures the average time per site visit (in seconds) divided by the largest order size. It could help identify whether customers who spend more time on the site tend to place larger orders.\n",
    "\n",
    "time_vs_total: This ratio measures the average time per site visit (in seconds) divided by the log of total meals ordered. It could help identify whether customers who spend more time on the site tend to order more meals overall.\n",
    "\n",
    "total_vs_average: This ratio measures the log of total meals ordered divided by the largest order size. It could help identify whether customers who order more meals tend to place larger orders.\n",
    "\n",
    "total_photos_vs_unique: This ratio measures the log of total photos viewed divided by the number of unique meals purchased. It could help identify whether customers who view more photos tend to try more unique meals.\n",
    "\n",
    "unique_total: calculates the number of unique meals per total meals ordered.\n",
    "\n",
    "small, below_medium, above_medium, large: calculates the type of user based on the number of unique meals ordered to total meals. The type of user is determined by quartiles.\n",
    "\n",
    "not_unique, unique, very_unique: calculates the type of user based on the number of unique meals. The type of user is determined by quartiles.\n",
    "\n",
    "not_active, active, very_active: calculates the type of user based on the number of mobile logins. The type of user is determined by quartiles.\n",
    "\n",
    "not_frequent, below_frequent, above_frequent, very_frequent: calculates the type of user based on the number of total orders. The type of user is determined by quartiles.\n",
    "\n",
    "The feature engineering steps performed on the 'chef' dataset can be used to identify patterns and relationships between the various features in the dataset. These relationships can be used to develop machine learning models that can predict various outcomes, such as customer satisfaction or revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97235ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### NEW FEATURES ############################## \n",
    "# how many unique meal purchases per mobile login\n",
    "chef['unique_per_mobile'] = chef['MOBILE_LOGINS'] / chef['UNIQUE_MEALS_PURCH']\n",
    "# how many unique meals purchased to cancellations after noon\n",
    "chef['cancel_per_unique'] = chef['UNIQUE_MEALS_PURCH'] / chef['log_CANCELLATIONS_AFTER_NOON']\n",
    "# how many mobile logins per total meals ordered \n",
    "chef['mobile_per_total'] = chef['MOBILE_LOGINS'] / chef['log_TOTAL_MEALS_ORDERED']\n",
    "# how much revenue per meals ordered \n",
    "chef['total_to_revenue']=chef['log_REVENUE']/chef['log_TOTAL_MEALS_ORDERED']\n",
    "# how many meals purchased to cancellations after \n",
    "chef['total_to_cancel']=chef['log_TOTAL_MEALS_ORDERED']/chef['log_CANCELLATIONS_AFTER_NOON']\n",
    "# how many mobile logins to pc logins\n",
    "chef['mobile_pc']=chef['MOBILE_LOGINS']/chef['PC_LOGINS']\n",
    "# how many meals ordered per late delivery\n",
    "chef['total_to_late']=chef['log_TOTAL_MEALS_ORDERED']/chef['log_LATE_DELIVERIES']\n",
    "\n",
    "# finding the ratio of average time per site visit to average order size \n",
    "chef['time_vs_average'] = chef['AVG_TIME_PER_SITE_VISIT'] / chef['LARGEST_ORDER_SIZE']\n",
    "# finding the ratio of average time per visit to total meals order\n",
    "chef['time_vs_total'] = chef['AVG_TIME_PER_SITE_VISIT'] / chef['log_TOTAL_MEALS_ORDERED']\n",
    "# finding the ratio of total meals ordred to average order size\n",
    "chef['total_vs_average'] = chef['log_TOTAL_MEALS_ORDERED'] / chef['LARGEST_ORDER_SIZE']\n",
    "# finding the ratio of photos viewed to unique meals purchased\n",
    "chef['total_photos_vs_unique'] = chef['log_TOTAL_PHOTOS_VIEWED'] / chef['UNIQUE_MEALS_PURCH']\n",
    "\n",
    "# how many unique meals per total meals ordered \n",
    "chef['unique_total'] = chef['UNIQUE_MEALS_PURCH'] / chef['log_TOTAL_MEALS_ORDERED']\n",
    "# calculating type of user based on the number of unique meals orderd to total meals \n",
    "chef['small'] = 0\n",
    "chef['below_medium'] = 0\n",
    "chef['above_medium'] = 0\n",
    "chef['large'] = 0\n",
    "# loop to identify type of user\n",
    "for index, row in chef.iterrows():\n",
    "    # small - 25th percentile\n",
    "    if chef.loc[index, 'unique_total'] < 1.29 :\n",
    "        chef.loc[index, 'small'] = 1   \n",
    "    # below medium - below 50th percentile\n",
    "    elif chef.loc[index, 'unique_total'] >= 1.29 and chef.loc[index, 'unique_total'] < 1.57: \n",
    "        chef.loc[index, 'below_medium'] = 1\n",
    "    # above medium - above 50th percentile\n",
    "    elif chef.loc[index, 'unique_total'] >= 1.57 and chef.loc[index, 'unique_total'] < 1.92: \n",
    "        chef.loc[index, 'above_medium'] = 1\n",
    "    # large - 75th percentile\n",
    "    elif chef.loc[index, 'unique_total'] >= 1.92 :\n",
    "        chef.loc[index, 'large'] = 1\n",
    "          \n",
    "# calculating type of user based on the number of unique meals\n",
    "chef['not_unique'] = 0\n",
    "chef['unique'] = 0\n",
    "chef['very_unique'] = 0\n",
    "# loop to identify type of user\n",
    "for index, row in chef.iterrows():\n",
    "    # not unique - 25th percentile\n",
    "    if chef.loc[index, 'UNIQUE_MEALS_PURCH'] < 5:\n",
    "        chef.loc[index, 'not_unique'] = 1   \n",
    "    # unique - IQ\n",
    "    elif chef.loc[index, 'UNIQUE_MEALS_PURCH'] >= 5 and chef.loc[index, 'UNIQUE_MEALS_PURCH'] < 8: \n",
    "        chef.loc[index, 'unique'] = 1\n",
    "    # unique - 75th percentile\n",
    "    elif chef.loc[index, 'UNIQUE_MEALS_PURCH'] >= 8:\n",
    "        chef.loc[index, 'very_unique'] = 1\n",
    "        \n",
    "# calculating type of user based on the number of mobile logins\n",
    "chef['not_active'] = 0\n",
    "chef['active'] = 0\n",
    "chef['very_active'] = 0\n",
    "# loop to identify type of user\n",
    "for index, row in chef.iterrows():\n",
    "    # not active - 25th percentile\n",
    "    if chef.loc[index, 'MOBILE_LOGINS'] < 1 :\n",
    "        chef.loc[index, 'not_active'] = 1   \n",
    "    # active - IQ\n",
    "    elif chef.loc[index, 'MOBILE_LOGINS'] >= 1 and chef.loc[index, 'MOBILE_LOGINS'] < 2: \n",
    "        chef.loc[index, 'active'] = 1\n",
    "    # very active - 75th percentile\n",
    "    elif chef.loc[index, 'MOBILE_LOGINS'] >= 2:\n",
    "        chef.loc[index, 'very_active'] = 1\n",
    "        \n",
    "# calculating type of user based on the number of total orders\n",
    "chef['not_frequent'] = 0\n",
    "chef['below_frequent'] = 0\n",
    "chef['above_frequent'] = 0\n",
    "chef['very_frequent'] = 0\n",
    "# loop to identify type of user\n",
    "for index, row in chef.iterrows():\n",
    "    # not frequent - 25th percentile\n",
    "    if chef.loc[index, 'log_TOTAL_MEALS_ORDERED'] < 3.66 :\n",
    "        chef.loc[index, 'not_frequent'] = 1   \n",
    "    # below frequent - below 50th percentile\n",
    "    elif chef.loc[index, 'log_TOTAL_MEALS_ORDERED'] >= 3.66 and chef.loc[index, 'log_TOTAL_MEALS_ORDERED'] < 4.09: \n",
    "        chef.loc[index, 'below_frequent'] = 1\n",
    "    # above frequent - above 50th percentile\n",
    "    elif chef.loc[index, 'log_TOTAL_MEALS_ORDERED'] >= 4.09 and chef.loc[index, 'log_TOTAL_MEALS_ORDERED'] < 4.55: \n",
    "        chef.loc[index, 'above_frequent'] = 1\n",
    "    # very frequent - 75th percentile\n",
    "    elif chef.loc[index, 'log_TOTAL_MEALS_ORDERED'] >= 4.55 :\n",
    "        chef.loc[index, 'very_frequent'] = 1\n",
    "        \n",
    "# calculating type of user based on the log revenue\n",
    "chef['low'] = 0\n",
    "chef['low_mid'] = 0\n",
    "chef['high_mid'] = 0\n",
    "chef['high'] = 0\n",
    "# loop to identify type of user\n",
    "for index, row in chef.iterrows():\n",
    "    # low - 25th percentile\n",
    "    if chef.loc[index, 'log_REVENUE'] < 7.21 :\n",
    "        chef.loc[index, 'low'] = 1   \n",
    "    # low mid - below 50th percentile\n",
    "    elif chef.loc[index, 'log_REVENUE'] >= 7.21 and chef.loc[index, 'log_REVENUE'] < 7.46: \n",
    "        chef.loc[index, 'low_mid'] = 1\n",
    "    # high mid - above 50th percentile\n",
    "    elif chef.loc[index, 'log_REVENUE'] >= 7.46 and chef.loc[index, 'log_REVENUE'] < 7.89: \n",
    "        chef.loc[index, 'high_mid'] = 1\n",
    "    # high - 75th percentile\n",
    "    elif chef.loc[index, 'log_REVENUE'] >= 7.89 :\n",
    "        chef.loc[index, 'high'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a2f497",
   "metadata": {},
   "source": [
    "First, the code splits the data into a training set and a testing set using stratification. Then, the code trains each model on the training set and tests its performance on the testing set.\n",
    "\n",
    "For each model, the code calculates the following performance metrics:\n",
    "\n",
    "Training score: the accuracy of the model on the training set\n",
    "Testing score: the accuracy of the model on the testing set\n",
    "Testing gap: the absolute difference between the training and testing scores\n",
    "AUC score: the area under the receiver operating characteristic (ROC) curve\n",
    "Confusion matrix: a table showing the number of true positives, true negatives, false positives, and false negatives for the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d19e728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Winning analysis:                                                                                       Random Forest                 \n",
      "                    ----------------------------------------------------------------------------------------------------\n",
      "Model Type:         Logistic Regression\tDecision Tree\tDecision Tree Pruned\tK Neighbors Classifier\tRandom Forest\n",
      "                    ----------------------------------------------------------------------------------------------------\n",
      "Training Accuracy:  0.679\t\t1.0\t\t0.7053\t\t\t0.7476\t\t\t0.7076\n",
      "Testing Accuracy:   0.6769\t\t0.5333\t\t0.6308\t\t\t0.6154\t\t\t0.6718\n",
      "Train Test Gap:     0.0021\t\t0.4667\t\t0.0745\t\t\t0.1322\t\t\t0.0358\n",
      "AUC:                0.5872\t\t0.4894\t\t0.5281\t\t\t0.5002\t\t\t0.6134\n",
      "\n",
      "\n",
      "Confusion Matrix:   \n",
      "True Negatives :    2\t\t\t23\t\t15\t\t\t11\t\t\t0\n",
      "False Positives:    61\t\t\t40\t\t48\t\t\t52\t\t\t63\n",
      "False Negatives:    2\t\t\t51\t\t24\t\t\t23\t\t\t1\n",
      "True Positives :    130\t\t\t81\t\t108\t\t\t109\t\t\t131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########################### NEW DATA ############################## \n",
    "# train/test split with the new model data \n",
    "chef_data   =  chef.loc[ : , ['log_CANCELLATIONS_AFTER_NOON', 'AVG_PREP_VID_TIME', \n",
    "                            'LARGEST_ORDER_SIZE', 'log_TOTAL_MEALS_ORDERED', \n",
    "                            'log_LATE_DELIVERIES', \n",
    "                            'cancel_per_unique', 'unique_per_mobile', \n",
    "                            'total_to_cancel',  'total_to_revenue', 'mobile_per_total',\n",
    "                            'MOBILE_LOGINS', 'mobile_pc', 'total_to_late', \n",
    "                            'below_medium', 'small', 'large', 'above_medium',\n",
    "                            'unique', 'very_unique', 'not_active', 'active', \n",
    "                            'very_active', 'not_frequent', 'below_frequent', \n",
    "                            'above_frequent', 'very_frequent', 'low', 'low_mid', \n",
    "                            'high_mid']]\n",
    "chef_target =  chef.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "# redo train/test split with stratification with new data \n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            chef_data,\n",
    "            chef_target,\n",
    "            test_size = 0.10,\n",
    "            random_state = 219,\n",
    "            stratify = chef_target)\n",
    "\n",
    "########################### LOGISTIC REGRESSION ############################## \n",
    "# name of model\n",
    "model_1 = \"Logistic Regression\"\n",
    "# inisiate logistic regression model\n",
    "logreg = LogisticRegression(max_iter = 200,\n",
    "                            solver = 'lbfgs',\n",
    "                            C = 1,\n",
    "                            random_state = 219)\n",
    "# fit the training data\n",
    "logreg_fit = logreg.fit(x_train, y_train)\n",
    "# predict based on the testing set\n",
    "logreg_pred = logreg.predict(x_test)\n",
    "logreg_pred_probs = logreg_fit.predict_proba(x_test)[:, 1]\n",
    "# score results\n",
    "logreg_train_score = logreg_fit.score(x_train, y_train).round(4) \n",
    "logreg_test_score = logreg_fit.score(x_test, y_test).round(4)\n",
    "logreg_test_gap = abs(logreg_train_score - logreg_test_score).round(4)\n",
    "logreg_auc_score = roc_auc_score(y_true = y_test, y_score = logreg_pred_probs).round(4)\n",
    "# confusion matrix\n",
    "logreg_tn, \\\n",
    "logreg_fp, \\\n",
    "logreg_fn, \\\n",
    "logreg_tp = confusion_matrix(y_true = y_test, y_pred = logreg_pred).ravel()\n",
    "\n",
    "########################### DECISION TREE ############################## \n",
    "# name of model\n",
    "model_2 = \"Decision Tree\"\n",
    "# initiate a classification tree object\n",
    "tree = DecisionTreeClassifier()\n",
    "# fit the training data\n",
    "tree_fit = tree.fit(x_train, y_train)\n",
    "# predict on new data\n",
    "tree_pred = tree.predict(x_test)\n",
    "tree_pred_probs = tree_fit.predict_proba(x_test)[:, 1]\n",
    "# score the model\n",
    "tree_train_score = tree_fit.score(x_train, y_train).round(4)\n",
    "tree_test_score  = tree_fit.score(x_test, y_test).round(4) \n",
    "tree_test_gap = abs(tree_train_score - tree_test_score).round(4)\n",
    "tree_auc_score = roc_auc_score(y_true  = y_test, y_score = tree_pred_probs).round(4)\n",
    "# confusion matrix\n",
    "tree_tn, \\\n",
    "tree_fp, \\\n",
    "tree_fn, \\\n",
    "tree_tp = confusion_matrix(y_true = y_test, y_pred = tree_pred).ravel()\n",
    "\n",
    "########################### PRUNED DECISION TREE ############################## \n",
    "# name of model\n",
    "model_3 = \"Decision Tree Pruned\"\n",
    "# initiate a classification tree \n",
    "pruned = DecisionTreeClassifier(max_depth = 6,\n",
    "                                min_samples_leaf = 10,\n",
    "                                random_state = 219)\n",
    "# fit the training data\n",
    "pruned_fit = pruned.fit(x_train, y_train)\n",
    "# predict on data\n",
    "pruned_pred = pruned.predict(x_test)\n",
    "pruned_pred_probs = pruned_fit.predict_proba(x_test)[:, 1]\n",
    "# score the model\n",
    "pruned_train_score = pruned_fit.score(x_train, y_train).round(4) \n",
    "pruned_test_score = pruned_fit.score(x_test, y_test).round(4)\n",
    "pruned_test_gap = abs(pruned_train_score - pruned_test_score).round(4)\n",
    "pruned_auc_score = roc_auc_score(y_true  = y_test, y_score = pruned_pred).round(4)\n",
    "# confusion matrix\n",
    "pruned_tn, \\\n",
    "pruned_fp, \\\n",
    "pruned_fn, \\\n",
    "pruned_tp = confusion_matrix(y_true = y_test, y_pred = pruned_pred).ravel()\n",
    "\n",
    "########################### KNN ############################## \n",
    "# name of model\n",
    "model_4 = \"K Neighbors Classifier\"\n",
    "# initiate a KNN classifier object\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "# fit the model to the training data\n",
    "knn_fit = knn.fit(x_train, y_train)\n",
    "# predict on the testing data\n",
    "knn_pred = knn.predict(x_test)\n",
    "knn_pred_probs = knn_fit.predict_proba(x_test)[:, 1]\n",
    "# score the model\n",
    "knn_train_score = knn_fit.score(x_train, y_train).round(4) \n",
    "knn_test_score = knn_fit.score(x_test, y_test).round(4)\n",
    "knn_test_gap = abs(knn_train_score - knn_test_score).round(4)\n",
    "knn_auc_score = roc_auc_score(y_true  = y_test, y_score = knn_pred).round(4)\n",
    "# confusion matrix\n",
    "knn_tn, \\\n",
    "knn_fp, \\\n",
    "knn_fn, \\\n",
    "knn_tp = confusion_matrix(y_true = y_test, y_pred = knn_pred).ravel()\n",
    "\n",
    "########################### RANDOM FOREST ########################\n",
    "# name of model\n",
    "model_5 = \"Random Forest\"\n",
    "# initiate a random forest\n",
    "rfc = RandomForestClassifier(n_estimators = 30,\n",
    "                             max_depth = 6,\n",
    "                             random_state = 219)\n",
    "# fit the model to the training data\n",
    "rfc_fit = rfc.fit(x_train, y_train)\n",
    "# predict on the testing data\n",
    "rfc_pred = rfc.predict(x_test)\n",
    "rfc_pred_probs = rfc_fit.predict_proba(x_test)[:, 1]\n",
    "# score the model\n",
    "rfc_train_score = rfc_fit.score(x_train, y_train).round(4) \n",
    "rfc_test_score = rfc_fit.score(x_test, y_test).round(4)\n",
    "rfc_test_gap = abs(rfc_train_score - rfc_test_score).round(4)\n",
    "rfc_auc_score = roc_auc_score(y_true  = y_test, y_score = rfc_pred_probs).round(4)\n",
    "# confusion matrix\n",
    "rfc_tn, \\\n",
    "rfc_fp, \\\n",
    "rfc_fn, \\\n",
    "rfc_tp = confusion_matrix(y_true = y_test, y_pred = rfc_pred).ravel()\n",
    "\n",
    "############################ Final Classification Results ##########################\n",
    "# printing dynamic results\n",
    "print(f\"\"\"\n",
    "Winning analysis:                                                                                       {model_5}                 \n",
    "                    ----------------------------------------------------------------------------------------------------\n",
    "Model Type:         {model_1}\\t{model_2}\\t{model_3}\\t{model_4}\\t{model_5}\n",
    "                    ----------------------------------------------------------------------------------------------------\n",
    "Training Accuracy:  {logreg_train_score}\\t\\t{tree_train_score}\\t\\t{pruned_train_score}\\t\\t\\t{knn_train_score}\\t\\t\\t{rfc_train_score}\n",
    "Testing Accuracy:   {logreg_test_score}\\t\\t{tree_test_score}\\t\\t{pruned_test_score}\\t\\t\\t{knn_test_score}\\t\\t\\t{rfc_test_score}\n",
    "Train Test Gap:     {logreg_test_gap}\\t\\t{tree_test_gap}\\t\\t{pruned_test_gap}\\t\\t\\t{knn_test_gap}\\t\\t\\t{rfc_test_gap}\n",
    "AUC:                {logreg_auc_score}\\t\\t{tree_auc_score}\\t\\t{pruned_auc_score}\\t\\t\\t{knn_auc_score}\\t\\t\\t{rfc_auc_score}\n",
    "\n",
    "\n",
    "Confusion Matrix:   \n",
    "True Negatives :    {logreg_tn}\\t\\t\\t{tree_tn}\\t\\t{pruned_tn}\\t\\t\\t{knn_tn}\\t\\t\\t{rfc_tn}\n",
    "False Positives:    {logreg_fp}\\t\\t\\t{tree_fp}\\t\\t{pruned_fp}\\t\\t\\t{knn_fp}\\t\\t\\t{rfc_fp}\n",
    "False Negatives:    {logreg_fn}\\t\\t\\t{tree_fn}\\t\\t{pruned_fn}\\t\\t\\t{knn_fn}\\t\\t\\t{rfc_fn}\n",
    "True Positives :    {logreg_tp}\\t\\t\\t{tree_tp}\\t\\t{pruned_tp}\\t\\t\\t{knn_tp}\\t\\t\\t{rfc_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa79cc",
   "metadata": {},
   "source": [
    "The code first identifies the independent variables (features) to use in the regression by selecting a subset of columns from the dataset. It then splits the data into training and testing sets using a 75:25 split.\n",
    "\n",
    "Three different regression models are then fit to the training data: Lasso Regression, Linear Regression, and ARD Regression. Each model is fit and tested, and the training and testing scores are recorded. Finally, the code prints out the results for each model, showing which model performed the best.\n",
    "\n",
    "The Lasso Regression model uses L1 regularization to shrink the coefficients of less important variables to zero, effectively performing feature selection. The Linear Regression model fits a linear equation to the data. The ARD Regression model uses a Bayesian approach to automatically determine the importance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9266e317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Winning analysis:                       Linear Regression\n",
      "                   ----------------------------------------------------------------\n",
      "Model Name:       | Lasso Regression\tLinear Regression\tARD Regression\n",
      "                   ----------------------------------------------------------------\n",
      "Train Score:      | 0.6573\t\t0.703\t\t\t0.6916\n",
      "Test Score:       | 0.6534\t\t0.7187\t\t\t0.7103\n",
      "Train Test Gap:   | 0.0039\t\t0.0039\t\t\t0.0187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########################## SCIKIT LEARN ################################\n",
    "# identify independent variables\n",
    "x_var = ['log_TOTAL_MEALS_ORDERED', 'log_TOTAL_PHOTOS_VIEWED', 'AVG_MEAN_RATING',\n",
    "               'LARGEST_ORDER_SIZE', 'AVG_PREP_VID_TIME', 'UNIQUE_MEALS_PURCH',\n",
    "               'CONTACTS_W_CUSTOMER_SERVICE', 'AVG_TIME_PER_SITE_VISIT', 'MOBILE_LOGINS',\n",
    "               'log_PRODUCT_CATEGORIES_VIEWED', 'time_vs_average', 'time_vs_total', \n",
    "               'total_vs_average', 'total_photos_vs_unique']\n",
    "\n",
    "# Preparing a DataFrame \n",
    "x_data = chef.loc[ : , x_var]\n",
    "\n",
    "# identify dependent variable\n",
    "y_data = chef.loc[ : , 'log_REVENUE']\n",
    "\n",
    "# train/test split up \n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            x_data, \n",
    "            y_data, \n",
    "            test_size    = 0.25,\n",
    "            random_state = 219)\n",
    "\n",
    "############################ lasso regression ##########################\n",
    "# set a model name\n",
    "model_1_name = \"Lasso Regression\"\n",
    "\n",
    "# specify a model object \n",
    "model_1 = sklearn.linear_model.Lasso(alpha=0.05)\n",
    "# fit to the training data\n",
    "model_1_fit = model_1.fit(x_train, y_train)\n",
    "\n",
    "# predict on new data\n",
    "model_1_pred = model_1.predict(x_test)\n",
    "\n",
    "# score the results\n",
    "model_1_train_score = model_1.score(x_train, y_train).round(4)\n",
    "model_1_test_score  = model_1.score(x_test, y_test).round(4) \n",
    "model_1_gap         = abs(model_1_train_score - model_1_test_score).round(4)\n",
    "\n",
    "############################ linear regression ##########################\n",
    "\n",
    "# Set a model name\n",
    "model_2_name = \"Linear Regression\"\n",
    "\n",
    "# Specify a model object \n",
    "model_2 = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# fit to the training data\n",
    "model_2_fit = model_2.fit(x_train, y_train)\n",
    "\n",
    "# predict on new data\n",
    "model_2_pred = model_2.predict(x_test)\n",
    "\n",
    "# score the results\n",
    "model_2_train_score = model_2.score(x_train, y_train).round(4) \n",
    "model_2_test_score  = model_2.score(x_test, y_test).round(4)   \n",
    "model_2_gap         = abs(model_2_train_score - model_2_test_score).round(4)\n",
    "\n",
    "############################ ARD regression ##########################\n",
    "\n",
    "# set a model name\n",
    "model_3_name = \"ARD Regression\"\n",
    "\n",
    "# specify a model object \n",
    "model_3 = sklearn.linear_model.ARDRegression()\n",
    "\n",
    "# fit to the training data\n",
    "model_3_fit = model_3.fit(x_train, y_train)\n",
    "2\n",
    "# predict on new data\n",
    "model_3_pred = model_3.predict(x_test)\n",
    "\n",
    "# score the results\n",
    "model_3_train_score = model_3.score(x_train, y_train).round(4)\n",
    "model_3_test_score  = model_3.score(x_test, y_test).round(4) \n",
    "model_3_gap         = abs(model_3_train_score - model_3_test_score).round(4)\n",
    "\n",
    "############################ Final Regression results ##########################\n",
    "# printing dynamic results\n",
    "print(f\"\"\"\n",
    "Winning analysis:                       {model_2_name}\n",
    "                   ----------------------------------------------------------------\n",
    "Model Name:       | {model_1_name}\\t{model_2_name}\\t{model_3_name}\n",
    "                   ----------------------------------------------------------------\n",
    "Train Score:      | {model_1_train_score}\\t\\t{model_2_train_score}\\t\\t\\t{model_3_train_score}\n",
    "Test Score:       | {model_1_test_score}\\t\\t{model_2_test_score}\\t\\t\\t{model_3_test_score}\n",
    "Train Test Gap:   | {model_1_gap}\\t\\t{model_1_gap}\\t\\t\\t{model_3_gap}\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
