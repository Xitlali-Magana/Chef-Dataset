{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "575c85eb",
   "metadata": {},
   "source": [
    "<hr style=\"height:.6px;color:#333;\" />\n",
    "<h1><b>Classification Model Development</b></h1>\n",
    "<h2>Xitlali Magana</h2>\n",
    "<br>\n",
    "<hr style=\"height:.3px;color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc9d60f",
   "metadata": {},
   "source": [
    "I begin by importing various libraries such as numpy, pandas, matplotlib, seaborn, and scikit-learn. I proceed to load the dataset named \"Cross_Sell_Success_Dataset_2023.xlsx\" using pandas' read_excel() function and assigns it to the variable css. Next, I set some pandas options to adjust the display of data frames. Finally, I display the first 5 rows of the loaded dataset using the head() function with a parameter of n=5. This allows me to get a quick preview of the data and ensure that it was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f008e212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CROSS_SELL_SUCCESS</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>REVENUE</th>\n",
       "      <th>TOTAL_MEALS_ORDERED</th>\n",
       "      <th>UNIQUE_MEALS_PURCH</th>\n",
       "      <th>CONTACTS_W_CUSTOMER_SERVICE</th>\n",
       "      <th>PRODUCT_CATEGORIES_VIEWED</th>\n",
       "      <th>AVG_TIME_PER_SITE_VISIT</th>\n",
       "      <th>CANCELLATIONS_AFTER_NOON</th>\n",
       "      <th>PC_LOGINS</th>\n",
       "      <th>MOBILE_LOGINS</th>\n",
       "      <th>WEEKLY_PLAN</th>\n",
       "      <th>LATE_DELIVERIES</th>\n",
       "      <th>AVG_PREP_VID_TIME</th>\n",
       "      <th>LARGEST_ORDER_SIZE</th>\n",
       "      <th>AVG_MEAN_RATING</th>\n",
       "      <th>TOTAL_PHOTOS_VIEWED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>steffon.baratheon@yahoo.com</td>\n",
       "      <td>4920.0</td>\n",
       "      <td>493</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>265.6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>137.41</td>\n",
       "      <td>6</td>\n",
       "      <td>2.894737</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>harlon.greyjoy@visa.com</td>\n",
       "      <td>6150.0</td>\n",
       "      <td>361</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>247.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120.20</td>\n",
       "      <td>5</td>\n",
       "      <td>2.631579</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>monster@protonmail.com</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>278</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>164.4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>127.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>damon.lannister.(lord)@yahoo.com</td>\n",
       "      <td>3330.0</td>\n",
       "      <td>269</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>176.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>129.78</td>\n",
       "      <td>6</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>raynald.westerling@jnj.com</td>\n",
       "      <td>3427.5</td>\n",
       "      <td>276</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>164.6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>34.42</td>\n",
       "      <td>3</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CROSS_SELL_SUCCESS                             EMAIL  REVENUE  TOTAL_MEALS_ORDERED  UNIQUE_MEALS_PURCH  CONTACTS_W_CUSTOMER_SERVICE  PRODUCT_CATEGORIES_VIEWED  AVG_TIME_PER_SITE_VISIT  CANCELLATIONS_AFTER_NOON  PC_LOGINS  MOBILE_LOGINS  WEEKLY_PLAN  LATE_DELIVERIES   AVG_PREP_VID_TIME  LARGEST_ORDER_SIZE  AVG_MEAN_RATING  TOTAL_PHOTOS_VIEWED\n",
       "0                   1       steffon.baratheon@yahoo.com   4920.0                  493                   9                            1                         10                    265.6                         5          5              2            0                 0             137.41                   6         2.894737                  456\n",
       "1                   0           harlon.greyjoy@visa.com   6150.0                  361                   9                            1                          6                    247.0                         2          5              1            0                 0             120.20                   5         2.631579                  680\n",
       "2                   0            monster@protonmail.com   3435.0                  278                   6                            1                          4                    164.4                         0          6              1            5                 0             127.00                   3         3.684211                  145\n",
       "3                   1  damon.lannister.(lord)@yahoo.com   3330.0                  269                   8                            1                          2                    176.0                         5          5              2            0                 0             129.78                   6         3.157895                  418\n",
       "4                   1        raynald.westerling@jnj.com   3427.5                  276                   7                            1                         10                    164.6                         0          6              1           14                 0              34.42                   3         3.157895                  174"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns    \n",
    "from sklearn.metrics import confusion_matrix         \n",
    "from sklearn.metrics import roc_auc_score          \n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.neighbors import KNeighborsRegressor   \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# loading data\n",
    "css = pd.read_excel(io = 'Cross_Sell_Success_Dataset_2023.xlsx')\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "# displaying the head of the dataset\n",
    "css.head(n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a578b8",
   "metadata": {},
   "source": [
    "In the following code I begin by cleaning the data. This includes checking for missing values, and column names. The features did not have any missing values, so nothing was done. For the column names, I changed the column LATE_DELIVERIES to remove a space at the end. This was done to avoid future complications. Once the data was cleaned and ready to use, I ran a for loop to create a histogram for every feature in the dataset. My reason for doing this was to see the distribution and ensure there was no skewness. However, this revealed that there were 7 features that were significantly skewed. To combat this, I created a list of these skewed features and ran another for loop to log them and create a new column for each one. Once this was done, I dropped the skewed columns and remained with the new log transformed ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9219b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### data cleaning ############################\n",
    "# checking each feature for missing values\n",
    "css.isnull().sum().round(decimals=2)\n",
    "# checking column names\n",
    "css.columns\n",
    "# renaming the late deliveries column to remove the space\n",
    "css = css.rename(columns={\"LATE_DELIVERIES \":\"LATE_DELIVERIES\"})\n",
    "\n",
    "####################### HISTPLOTS ############################\n",
    "# Iterating over the columns in the css DataFrame\n",
    "\"\"\" (commenting out becuase only needed for visualization)\n",
    "for col in css.columns:\n",
    "    # creating a histplot for every column\n",
    "    sns.histplot(data=css, \n",
    "                 x=col, \n",
    "                 kde=False)\n",
    "    # adding a title to each histogram\n",
    "    plt.title(f\"Histogram of {col}\")\n",
    "    # displaying the histogram\n",
    "    # plt.show() \n",
    "\"\"\"\n",
    "    \n",
    "####################### LOG TRANSFORMATIONS ############################\n",
    "# list of columns that appear to be skewed\n",
    "skewed_columns = ['REVENUE', 'TOTAL_MEALS_ORDERED', 'PRODUCT_CATEGORIES_VIEWED',\n",
    "                  'CANCELLATIONS_AFTER_NOON', 'WEEKLY_PLAN', 'LATE_DELIVERIES',\n",
    "                 'TOTAL_PHOTOS_VIEWED']\n",
    "# for loop to create a new column for each skewed column with a log transformation\n",
    "for col in skewed_columns:\n",
    "    if col in css.columns:\n",
    "        css['log_' + col] = np.log(css[col] + 0.001)\n",
    "        \n",
    "########################### DATA DROP ############################## \n",
    "# dropping useless features, and duplicates\n",
    "css = css.drop(['EMAIL', 'REVENUE', 'TOTAL_MEALS_ORDERED', 'PRODUCT_CATEGORIES_VIEWED',\n",
    "                'CANCELLATIONS_AFTER_NOON', 'WEEKLY_PLAN', 'LATE_DELIVERIES', \n",
    "                'TOTAL_PHOTOS_VIEWED'],\n",
    "               axis = 1)\n",
    "# checking results\n",
    "# css.columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a1c82",
   "metadata": {},
   "source": [
    "I began the following section of code by running a correlation of the features. I did so by calculating the correlation of the features using Pearson's correlation coefficient and rounding the values to 2 decimal places. Then, sorting the correlations in descending order for the target variable \"CROSS_SELL_SUCCESS\". The reason for this is to see which features impacted CROSS_SELL_SUCCESS the most. I then prepared the data for analysis by creating a list of the x variables, as well as identifying the y variable. I then used these variables to complete a train test split with the CROSS_SELL_SUCCESS, or y variable, stratified. I concluded with merging the training data to create a statsmodel. Seeing as the y variable of the data is binary, with only 0s and 1s, I used a logistic regression model instead of a linear one. This model served as a base model for future analysis and determination of the importance of the various features. I began by including all of the x variables and seeing the p-value. I proceeded to remove the feature with the highest p value and rerun the model. I continued to do this until all of the remaining features had a p value below 0.05. This resulted with leaving only 4 features, which I identified as most impactful in CROSS_SELL_SUCCESS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1d19bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.617189\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>Logit</td>       <td>Pseudo R-squared:</td>    <td>0.017</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td> <td>CROSS_SELL_SUCCESS</td>       <td>AIC:</td>         <td>2171.3948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>         <td>2023-02-19 21:08</td>        <td>BIC:</td>         <td>2198.7345</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>1751</td>         <td>Log-Likelihood:</td>    <td>-1080.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>               <td>4</td>             <td>LL-Null:</td>        <td>-1098.9</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>1746</td>          <td>LLR p-value:</td>    <td>2.3633e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>            <td>1.0000</td>            <td>Scale:</td>         <td>1.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>         <td>5.0000</td>               <td></td>               <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                <td></td>                <th>Coef.</th>  <th>Std.Err.</th>    <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                    <td>-0.5545</td>  <td>0.4151</td>  <td>-1.3359</td> <td>0.1816</td> <td>-1.3681</td> <td>0.2590</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UNIQUE_MEALS_PURCH</th>           <td>0.0548</td>   <td>0.0237</td>  <td>2.3153</td>  <td>0.0206</td> <td>0.0084</td>  <td>0.1013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MOBILE_LOGINS</th>                <td>0.2409</td>   <td>0.0983</td>  <td>2.4496</td>  <td>0.0143</td> <td>0.0481</td>  <td>0.4336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_TOTAL_MEALS_ORDERED</th>      <td>0.1697</td>   <td>0.0763</td>  <td>2.2247</td>  <td>0.0261</td> <td>0.0202</td>  <td>0.3191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_CANCELLATIONS_AFTER_NOON</th> <td>0.0676</td>   <td>0.0145</td>  <td>4.6527</td>  <td>0.0000</td> <td>0.0391</td>  <td>0.0961</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                              Results: Logit\n",
       "===========================================================================\n",
       "Model:                  Logit                Pseudo R-squared:   0.017     \n",
       "Dependent Variable:     CROSS_SELL_SUCCESS   AIC:                2171.3948 \n",
       "Date:                   2023-02-19 21:08     BIC:                2198.7345 \n",
       "No. Observations:       1751                 Log-Likelihood:     -1080.7   \n",
       "Df Model:               4                    LL-Null:            -1098.9   \n",
       "Df Residuals:           1746                 LLR p-value:        2.3633e-07\n",
       "Converged:              1.0000               Scale:              1.0000    \n",
       "No. Iterations:         5.0000                                             \n",
       "---------------------------------------------------------------------------\n",
       "                              Coef.  Std.Err.    z    P>|z|   [0.025 0.975]\n",
       "---------------------------------------------------------------------------\n",
       "Intercept                    -0.5545   0.4151 -1.3359 0.1816 -1.3681 0.2590\n",
       "UNIQUE_MEALS_PURCH            0.0548   0.0237  2.3153 0.0206  0.0084 0.1013\n",
       "MOBILE_LOGINS                 0.2409   0.0983  2.4496 0.0143  0.0481 0.4336\n",
       "log_TOTAL_MEALS_ORDERED       0.1697   0.0763  2.2247 0.0261  0.0202 0.3191\n",
       "log_CANCELLATIONS_AFTER_NOON  0.0676   0.0145  4.6527 0.0000  0.0391 0.0961\n",
       "===========================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################### CORRELATION ############################## \n",
    "# correlation of the features\n",
    "css_corr = css.corr(method=\"pearson\").round(decimals =2)\n",
    "css_corr['CROSS_SELL_SUCCESS'].sort_values(ascending = False)\n",
    "\n",
    "########################### PREPARING DATA ############################## \n",
    "# list of x variables\n",
    "x_var = ['UNIQUE_MEALS_PURCH', 'CONTACTS_W_CUSTOMER_SERVICE', 'AVG_TIME_PER_SITE_VISIT', \n",
    "          'PC_LOGINS', 'MOBILE_LOGINS', 'AVG_PREP_VID_TIME', 'LARGEST_ORDER_SIZE', \n",
    "          'AVG_MEAN_RATING', 'log_REVENUE', 'log_TOTAL_MEALS_ORDERED', \n",
    "          'log_PRODUCT_CATEGORIES_VIEWED', 'log_CANCELLATIONS_AFTER_NOON', \n",
    "          'log_WEEKLY_PLAN', 'log_LATE_DELIVERIES', 'log_TOTAL_PHOTOS_VIEWED']\n",
    "# declare explanatory variables (x)\n",
    "x_data = css[x_var]\n",
    "# declare response variable (y)\n",
    "y_var = css.loc[:, 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "########################### TEST/TRAIN SPLIT ############################## \n",
    "# train-test split including stratification\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            x_data,\n",
    "            y_var,\n",
    "            test_size = 0.10,\n",
    "            random_state = 219,\n",
    "            stratify = y_var) # preserving balance\n",
    "# merge training data for statsmodels\n",
    "css_train = pd.concat([x_train, y_train], axis = 1)\n",
    "\n",
    "########################### BASE MODEL ############################## \n",
    "# instantiating a logistic regression model object\n",
    "logistic = smf.logit(formula = \"\"\" CROSS_SELL_SUCCESS ~ UNIQUE_MEALS_PURCH + \n",
    "                                                     MOBILE_LOGINS +\n",
    "                                                     log_TOTAL_MEALS_ORDERED + \n",
    "                                                     log_CANCELLATIONS_AFTER_NOON \n",
    "                                                     \"\"\",\n",
    "                                        data = css_train)\n",
    "# fitting the model object\n",
    "results = logistic.fit()\n",
    "# checking the results SUMMARY\n",
    "results.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3465b78",
   "metadata": {},
   "source": [
    "To increase the potential scores of the models, I engineered three features. Using the results of the previous logistic regression I used the following features during engineering process: MOBILE_LOGINS, UNIQUE_MEALS_PURCH, log_CANCELLATIONS_AFTER_NOON, log_TOTAL_MEALS_ORDERED. \n",
    "\n",
    "The first feature that I created was to determine how many unique meals are purchased per mobile login. I created this feature because I assumed that the more logins the customer had the more likely the customer would purchase unique meals. \n",
    "The second feature that I engineered was to determine how many cancellations after noon per unique meals purchased. The reason I engineered this was because I assumed that customers who purchase unique orders are more skeptical and likely to cancel because it is not a product that the customer is accustomed to ordering. \n",
    "The third feature I engineered was to determine how many mobile logins per total meals ordered. The reasons for engineering this was because I assumed that the more purchases the customer makes the higher the probability that they logged in.\n",
    "The fourth feature determined how much revenue increased for a single meal order. I found this to be useful to see how each order impacted revenue, and as a result the cross_sell.\n",
    "The fifth feature was to see how many purchases result in a meal cancellation. I believed that the more meals purchased, the higher the chance of a cancellation. \n",
    "The sixth feature I created was to compare mobile logins to pc ones. I believe that the ratio will be much higher mobile ones per a single pc login.\n",
    "The seventh feature engineered was to compare the number of meals ordered per late delivery. I feel as if the higher the number of orders, the higher the number of late deliveries just because the chance of occurrence increases.\n",
    "\n",
    "The following features were created based on the results on the describe function.\n",
    "The eighth feature I engineered split the customers into 1 of 4 groups based on the number of unique meals per total meals ordered. The groups were small, below medium, above medium, and large. I felt like splitting them into these groups would further help the models, and I found that total meals and unique orders were significant.\n",
    "The ninth feature was used to determine the type of user based on the number of unique orders. They were split into one of three groups: not unique, unique, and very unique. The reason behind this was because unique orders was a significant feature, and I believe separating them into groups would help the models more than simply using the feature.\n",
    "The tenth feature I engineered split the users into one of four groups: not active, active, and very active. This was based on the number of mobile logins. The reason behind this was because mobile logins were significant in the statsmodel. I felt that this group precision was more beneficial that just the feature itself. \n",
    "The eleventh feature I engineered was determined off of the number of total orders, where users were split into one of these groups: not frequent, below frequent, above frequent, and very frequent. This was done for the reason that total order numbers was significant, and this method of using the data helped the models. \n",
    "The twelfth feature engineered grouped users based on their revenue. I felt like this would impact cross sell a significant amount.  \n",
    "\n",
    "Once all of the features were engineered I established all of the data for the classification models. I began by creating a list of all of the \"x\" variables I found to be important as well as the features I engineered. I then established the \"Y\" variable to be \"cross_cell_success\". I then used these variables to conduct a test train split with the Y variable being stratified. I performed trial and error to determine which features to include and which not to. I ended with the majority of the original features not used for the models. I also excluded some of the engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97235ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### NEW FEATURES ############################## \n",
    "# how many unique meal purchases per mobile login\n",
    "css['unique_per_mobile'] = css['MOBILE_LOGINS'] / css['UNIQUE_MEALS_PURCH']\n",
    "# how many unique meals purchased to cancellations after noon\n",
    "css['cancel_per_unique'] = css['UNIQUE_MEALS_PURCH'] / css['log_CANCELLATIONS_AFTER_NOON']\n",
    "# how many mobile logins per total meals ordered \n",
    "css['mobile_per_total'] = css['MOBILE_LOGINS'] / css['log_TOTAL_MEALS_ORDERED']\n",
    "# how much revenue per meals ordered \n",
    "css['total_to_revenue']=css['log_REVENUE']/css['log_TOTAL_MEALS_ORDERED']\n",
    "# how many meals purchased to cancellations after \n",
    "css['total_to_cancel']=css['log_TOTAL_MEALS_ORDERED']/css['log_CANCELLATIONS_AFTER_NOON']\n",
    "# how many mobile logins to pc logins\n",
    "css['mobile_pc']=css['MOBILE_LOGINS']/css['PC_LOGINS']\n",
    "# how many meals ordered per late delivery\n",
    "css['total_to_late']=css['log_TOTAL_MEALS_ORDERED']/css['log_LATE_DELIVERIES']\n",
    "\n",
    "# how many unique meals per total meals ordered \n",
    "css['unique_total'] = css['UNIQUE_MEALS_PURCH'] / css['log_TOTAL_MEALS_ORDERED']\n",
    "# calculating type of user based on the number of unique meals orderd to total meals \n",
    "css['small'] = 0\n",
    "css['below_medium'] = 0\n",
    "css['above_medium'] = 0\n",
    "css['large'] = 0\n",
    "# loop to identify type of user\n",
    "for index, row in css.iterrows():\n",
    "    # small - 25th percentile\n",
    "    if css.loc[index, 'unique_total'] < 1.29 :\n",
    "        css.loc[index, 'small'] = 1   \n",
    "    # below medium - below 50th percentile\n",
    "    elif css.loc[index, 'unique_total'] >= 1.29 and css.loc[index, 'unique_total'] < 1.57: \n",
    "        css.loc[index, 'below_medium'] = 1\n",
    "    # above medium - above 50th percentile\n",
    "    elif css.loc[index, 'unique_total'] >= 1.57 and css.loc[index, 'unique_total'] < 1.92: \n",
    "        css.loc[index, 'above_medium'] = 1\n",
    "    # large - 75th percentile\n",
    "    elif css.loc[index, 'unique_total'] >= 1.92 :\n",
    "        css.loc[index, 'large'] = 1\n",
    "          \n",
    "# calculating type of user based on the number of unique meals\n",
    "css['not_unique'] = 0\n",
    "css['unique'] = 0\n",
    "css['very_unique'] = 0\n",
    "# loop to identify type of user\n",
    "for index, row in css.iterrows():\n",
    "    # not unique - 25th percentile\n",
    "    if css.loc[index, 'UNIQUE_MEALS_PURCH'] < 5:\n",
    "        css.loc[index, 'not_unique'] = 1   \n",
    "    # unique - IQ\n",
    "    elif css.loc[index, 'UNIQUE_MEALS_PURCH'] >= 5 and css.loc[index, 'UNIQUE_MEALS_PURCH'] < 8: \n",
    "        css.loc[index, 'unique'] = 1\n",
    "    # unique - 75th percentile\n",
    "    elif css.loc[index, 'UNIQUE_MEALS_PURCH'] >= 8:\n",
    "        css.loc[index, 'very_unique'] = 1\n",
    "        \n",
    "# calculating type of user based on the number of mobile logins\n",
    "css['not_active'] = 0\n",
    "css['active'] = 0\n",
    "css['very_active'] = 0\n",
    "# loop to identify type of user\n",
    "for index, row in css.iterrows():\n",
    "    # not active - 25th percentile\n",
    "    if css.loc[index, 'MOBILE_LOGINS'] < 1 :\n",
    "        css.loc[index, 'not_active'] = 1   \n",
    "    # active - IQ\n",
    "    elif css.loc[index, 'MOBILE_LOGINS'] >= 1 and css.loc[index, 'MOBILE_LOGINS'] < 2: \n",
    "        css.loc[index, 'active'] = 1\n",
    "    # very active - 75th percentile\n",
    "    elif css.loc[index, 'MOBILE_LOGINS'] >= 2:\n",
    "        css.loc[index, 'very_active'] = 1\n",
    "        \n",
    "# calculating type of user based on the number of total orders\n",
    "css['not_frequent'] = 0\n",
    "css['below_frequent'] = 0\n",
    "css['above_frequent'] = 0\n",
    "css['very_frequent'] = 0\n",
    "# loop to identify type of user\n",
    "for index, row in css.iterrows():\n",
    "    # not frequent - 25th percentile\n",
    "    if css.loc[index, 'log_TOTAL_MEALS_ORDERED'] < 3.66 :\n",
    "        css.loc[index, 'not_frequent'] = 1   \n",
    "    # below frequent - below 50th percentile\n",
    "    elif css.loc[index, 'log_TOTAL_MEALS_ORDERED'] >= 3.66 and css.loc[index, 'log_TOTAL_MEALS_ORDERED'] < 4.09: \n",
    "        css.loc[index, 'below_frequent'] = 1\n",
    "    # above frequent - above 50th percentile\n",
    "    elif css.loc[index, 'log_TOTAL_MEALS_ORDERED'] >= 4.09 and css.loc[index, 'log_TOTAL_MEALS_ORDERED'] < 4.55: \n",
    "        css.loc[index, 'above_frequent'] = 1\n",
    "    # very frequent - 75th percentile\n",
    "    elif css.loc[index, 'log_TOTAL_MEALS_ORDERED'] >= 4.55 :\n",
    "        css.loc[index, 'very_frequent'] = 1\n",
    "        \n",
    "# calculating type of user based on the log revenue\n",
    "css['low'] = 0\n",
    "css['low_mid'] = 0\n",
    "css['high_mid'] = 0\n",
    "css['high'] = 0\n",
    "# loop to identify type of user\n",
    "for index, row in css.iterrows():\n",
    "    # low - 25th percentile\n",
    "    if css.loc[index, 'log_REVENUE'] < 7.21 :\n",
    "        css.loc[index, 'low'] = 1   \n",
    "    # low mid - below 50th percentile\n",
    "    elif css.loc[index, 'log_REVENUE'] >= 7.21 and css.loc[index, 'log_REVENUE'] < 7.46: \n",
    "        css.loc[index, 'low_mid'] = 1\n",
    "    # high mid - above 50th percentile\n",
    "    elif css.loc[index, 'log_REVENUE'] >= 7.46 and css.loc[index, 'log_REVENUE'] < 7.89: \n",
    "        css.loc[index, 'high_mid'] = 1\n",
    "    # high - 75th percentile\n",
    "    elif css.loc[index, 'log_REVENUE'] >= 7.89 :\n",
    "        css.loc[index, 'high'] = 1\n",
    "        \n",
    "########################### NEW DATA ############################## \n",
    "# train/test split with the new model data \n",
    "css_data   =  css.loc[ : , ['log_CANCELLATIONS_AFTER_NOON', 'AVG_PREP_VID_TIME', \n",
    "                            'LARGEST_ORDER_SIZE', 'log_TOTAL_MEALS_ORDERED', \n",
    "                            'log_LATE_DELIVERIES', \n",
    "                            'cancel_per_unique', 'unique_per_mobile', \n",
    "                            'total_to_cancel',  'total_to_revenue', 'mobile_per_total',\n",
    "                            'MOBILE_LOGINS', 'mobile_pc', 'total_to_late', \n",
    "                            'below_medium', 'small', 'large', 'above_medium',\n",
    "                            'unique', 'very_unique', 'not_active', 'active', \n",
    "                            'very_active', 'not_frequent', 'below_frequent', \n",
    "                            'above_frequent', 'very_frequent', 'low', 'low_mid', \n",
    "                            'high_mid']]\n",
    "css_target =  css.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "# redo train/test split with stratification with new data \n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            css_data,\n",
    "            css_target,\n",
    "            test_size = 0.10,\n",
    "            random_state = 219,\n",
    "            stratify = css_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a2f497",
   "metadata": {},
   "source": [
    "In the following block of code I ran five classification models: LOGISTIC REGRESSION, DECISION TREE, PRUNED DECISION TREE, K NEIGHBORS CLASSIFIER, and RANDOM FOREST. I also included the AUC score, and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d19e728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Winning analysis:                                                                                       Random Forest                 \n",
      "                    ----------------------------------------------------------------------------------------------------\n",
      "Model Type:         Logistic Regression\tDecision Tree\tDecision Tree Pruned\tK Neighbors Classifier\tRandom Forest\n",
      "                    ----------------------------------------------------------------------------------------------------\n",
      "Training Accuracy:  0.679\t\t1.0\t\t0.7053\t\t\t0.7476\t\t\t0.7076\n",
      "Testing Accuracy:   0.6769\t\t0.5179\t\t0.6308\t\t\t0.6154\t\t\t0.6718\n",
      "Train Test Gap:     0.0021\t\t0.4821\t\t0.0745\t\t\t0.1322\t\t\t0.0358\n",
      "AUC:                0.5872\t\t0.4738\t\t0.5281\t\t\t0.5002\t\t\t0.6134\n",
      "\n",
      "\n",
      "Confusion Matrix:   \n",
      "True Negatives :    2\t\t\t22\t\t15\t\t\t11\t\t\t0\n",
      "False Positives:    61\t\t\t41\t\t48\t\t\t52\t\t\t63\n",
      "False Negatives:    2\t\t\t53\t\t24\t\t\t23\t\t\t1\n",
      "True Positives :    130\t\t\t79\t\t108\t\t\t109\t\t\t131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########################### LOGISTIC REGRESSION ############################## \n",
    "# name of model\n",
    "model_1 = \"Logistic Regression\"\n",
    "# inisiate logistic regression model\n",
    "logreg = LogisticRegression(max_iter = 200,\n",
    "                            solver = 'lbfgs',\n",
    "                            C = 1,\n",
    "                            random_state = 219)\n",
    "# fit the training data\n",
    "logreg_fit = logreg.fit(x_train, y_train)\n",
    "# predict based on the testing set\n",
    "logreg_pred = logreg.predict(x_test)\n",
    "logreg_pred_probs = logreg_fit.predict_proba(x_test)[:, 1]\n",
    "# score results\n",
    "logreg_train_score = logreg_fit.score(x_train, y_train).round(4) \n",
    "logreg_test_score = logreg_fit.score(x_test, y_test).round(4)\n",
    "logreg_test_gap = abs(logreg_train_score - logreg_test_score).round(4)\n",
    "logreg_auc_score = roc_auc_score(y_true = y_test, y_score = logreg_pred_probs).round(4)\n",
    "# confusion matrix\n",
    "logreg_tn, \\\n",
    "logreg_fp, \\\n",
    "logreg_fn, \\\n",
    "logreg_tp = confusion_matrix(y_true = y_test, y_pred = logreg_pred).ravel()\n",
    "\n",
    "########################### DECISION TREE ############################## \n",
    "# name of model\n",
    "model_2 = \"Decision Tree\"\n",
    "# initiate a classification tree object\n",
    "tree = DecisionTreeClassifier()\n",
    "# fit the training data\n",
    "tree_fit = tree.fit(x_train, y_train)\n",
    "# predict on new data\n",
    "tree_pred = tree.predict(x_test)\n",
    "tree_pred_probs = tree_fit.predict_proba(x_test)[:, 1]\n",
    "# score the model\n",
    "tree_train_score = tree_fit.score(x_train, y_train).round(4)\n",
    "tree_test_score  = tree_fit.score(x_test, y_test).round(4) \n",
    "tree_test_gap = abs(tree_train_score - tree_test_score).round(4)\n",
    "tree_auc_score = roc_auc_score(y_true  = y_test, y_score = tree_pred_probs).round(4)\n",
    "# confusion matrix\n",
    "tree_tn, \\\n",
    "tree_fp, \\\n",
    "tree_fn, \\\n",
    "tree_tp = confusion_matrix(y_true = y_test, y_pred = tree_pred).ravel()\n",
    "\n",
    "########################### PRUNED DECISION TREE ############################## \n",
    "# name of model\n",
    "model_3 = \"Decision Tree Pruned\"\n",
    "# initiate a classification tree \n",
    "pruned = DecisionTreeClassifier(max_depth = 6,\n",
    "                                min_samples_leaf = 10,\n",
    "                                random_state = 219)\n",
    "# fit the training data\n",
    "pruned_fit = pruned.fit(x_train, y_train)\n",
    "# predict on data\n",
    "pruned_pred = pruned.predict(x_test)\n",
    "pruned_pred_probs = pruned_fit.predict_proba(x_test)[:, 1]\n",
    "# score the model\n",
    "pruned_train_score = pruned_fit.score(x_train, y_train).round(4) \n",
    "pruned_test_score = pruned_fit.score(x_test, y_test).round(4)\n",
    "pruned_test_gap = abs(pruned_train_score - pruned_test_score).round(4)\n",
    "pruned_auc_score = roc_auc_score(y_true  = y_test, y_score = pruned_pred).round(4)\n",
    "# confusion matrix\n",
    "pruned_tn, \\\n",
    "pruned_fp, \\\n",
    "pruned_fn, \\\n",
    "pruned_tp = confusion_matrix(y_true = y_test, y_pred = pruned_pred).ravel()\n",
    "\n",
    "########################### KNN ############################## \n",
    "# name of model\n",
    "model_4 = \"K Neighbors Classifier\"\n",
    "# initiate a KNN classifier object\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "# fit the model to the training data\n",
    "knn_fit = knn.fit(x_train, y_train)\n",
    "# predict on the testing data\n",
    "knn_pred = knn.predict(x_test)\n",
    "knn_pred_probs = knn_fit.predict_proba(x_test)[:, 1]\n",
    "# score the model\n",
    "knn_train_score = knn_fit.score(x_train, y_train).round(4) \n",
    "knn_test_score = knn_fit.score(x_test, y_test).round(4)\n",
    "knn_test_gap = abs(knn_train_score - knn_test_score).round(4)\n",
    "knn_auc_score = roc_auc_score(y_true  = y_test, y_score = knn_pred).round(4)\n",
    "# confusion matrix\n",
    "knn_tn, \\\n",
    "knn_fp, \\\n",
    "knn_fn, \\\n",
    "knn_tp = confusion_matrix(y_true = y_test, y_pred = knn_pred).ravel()\n",
    "\n",
    "########################### RANDOM FOREST ########################\n",
    "# name of model\n",
    "model_5 = \"Random Forest\"\n",
    "# initiate a random forest\n",
    "rfc = RandomForestClassifier(n_estimators = 30,\n",
    "                             max_depth = 6,\n",
    "                             random_state = 219)\n",
    "# fit the model to the training data\n",
    "rfc_fit = rfc.fit(x_train, y_train)\n",
    "# predict on the testing data\n",
    "rfc_pred = rfc.predict(x_test)\n",
    "rfc_pred_probs = rfc_fit.predict_proba(x_test)[:, 1]\n",
    "# score the model\n",
    "rfc_train_score = rfc_fit.score(x_train, y_train).round(4) \n",
    "rfc_test_score = rfc_fit.score(x_test, y_test).round(4)\n",
    "rfc_test_gap = abs(rfc_train_score - rfc_test_score).round(4)\n",
    "rfc_auc_score = roc_auc_score(y_true  = y_test, y_score = rfc_pred_probs).round(4)\n",
    "# confusion matrix\n",
    "rfc_tn, \\\n",
    "rfc_fp, \\\n",
    "rfc_fn, \\\n",
    "rfc_tp = confusion_matrix(y_true = y_test, y_pred = rfc_pred).ravel()\n",
    "\n",
    "############################ Final results ##########################\n",
    "# printing dynamic results\n",
    "print(f\"\"\"\n",
    "Winning analysis:                                                                                       {model_5}                 \n",
    "                    ----------------------------------------------------------------------------------------------------\n",
    "Model Type:         {model_1}\\t{model_2}\\t{model_3}\\t{model_4}\\t{model_5}\n",
    "                    ----------------------------------------------------------------------------------------------------\n",
    "Training Accuracy:  {logreg_train_score}\\t\\t{tree_train_score}\\t\\t{pruned_train_score}\\t\\t\\t{knn_train_score}\\t\\t\\t{rfc_train_score}\n",
    "Testing Accuracy:   {logreg_test_score}\\t\\t{tree_test_score}\\t\\t{pruned_test_score}\\t\\t\\t{knn_test_score}\\t\\t\\t{rfc_test_score}\n",
    "Train Test Gap:     {logreg_test_gap}\\t\\t{tree_test_gap}\\t\\t{pruned_test_gap}\\t\\t\\t{knn_test_gap}\\t\\t\\t{rfc_test_gap}\n",
    "AUC:                {logreg_auc_score}\\t\\t{tree_auc_score}\\t\\t{pruned_auc_score}\\t\\t\\t{knn_auc_score}\\t\\t\\t{rfc_auc_score}\n",
    "\n",
    "\n",
    "Confusion Matrix:   \n",
    "True Negatives :    {logreg_tn}\\t\\t\\t{tree_tn}\\t\\t{pruned_tn}\\t\\t\\t{knn_tn}\\t\\t\\t{rfc_tn}\n",
    "False Positives:    {logreg_fp}\\t\\t\\t{tree_fp}\\t\\t{pruned_fp}\\t\\t\\t{knn_fp}\\t\\t\\t{rfc_fp}\n",
    "False Negatives:    {logreg_fn}\\t\\t\\t{tree_fn}\\t\\t{pruned_fn}\\t\\t\\t{knn_fn}\\t\\t\\t{rfc_fn}\n",
    "True Positives :    {logreg_tp}\\t\\t\\t{tree_tp}\\t\\t{pruned_tp}\\t\\t\\t{knn_tp}\\t\\t\\t{rfc_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9266e317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
